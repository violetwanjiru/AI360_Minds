{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d95775a"
      },
      "source": [
        "# Task\n",
        "Train a lightweight image classification model using TensorFlow Lite for recognizing recyclable items using the dataset provided in \"/content/waste.zip\". Convert the model to TensorFlow Lite, test it, and explain the benefits of Edge AI for real-time applications. Deliverables: Code and a report with accuracy metrics and deployment steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26d8eef5"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the image dataset from \"/content/waste.zip\". This will involve extracting the zip file, organizing the data, and preparing it for use with TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5102db9a"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the zip file containing the image dataset and load the training and validation datasets using `image_dataset_from_directory`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9078a8",
        "outputId": "05716867-9faf-4e2d-c9c1-01d9909e34fb"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a directory to extract the zip file\n",
        "extract_dir = '/content/waste'\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile('/content/waste.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Define the path to the extracted dataset\n",
        "dataset_dir = os.path.join(extract_dir, 'waste') # Assuming the zip contains a 'waste' folder\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Get the class names\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15000 files belonging to 1 classes.\n",
            "Using 12000 files for training.\n",
            "Found 15000 files belonging to 1 classes.\n",
            "Using 3000 files for validation.\n",
            "Class names: ['images']\n",
            "Number of classes: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77107f50"
      },
      "source": [
        "**Reasoning**:\n",
        "The dataset loading seems to have a problem as it found only 1 class instead of 30. This is likely due to how the data is organized within the zip file. Need to inspect the extracted directory structure to correctly load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd3b8118",
        "outputId": "727d4730-2d02-46c4-8c6b-b0da47f93818"
      },
      "source": [
        "# Inspect the directory structure\n",
        "!ls -l /content/waste/waste\n",
        "\n",
        "# Re-attempt loading the dataset after inspecting the structure\n",
        "# Assuming the subdirectories containing images for each class are directly under /content/waste/waste\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Get the class names again\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 3 root root 4096 Jul 24 12:50 images\n",
            "-rw-r--r-- 1 root root 2775 Jul 24 13:59 README.txt\n",
            "Found 15000 files belonging to 1 classes.\n",
            "Using 12000 files for training.\n",
            "Found 15000 files belonging to 1 classes.\n",
            "Using 3000 files for validation.\n",
            "Class names: ['images']\n",
            "Number of classes: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ce0859"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the dataset still resulted in only one class being found. The directory listing shows a single 'images' directory under `/content/waste/waste`. This indicates that the actual class subdirectories are likely within this 'images' directory. I need to adjust the `dataset_dir` to point to `/content/waste/waste/images` to correctly load the data based on the subdirectories within 'images'. After correcting the path, I will apply a simple normalization preprocessing step to the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875180c9",
        "outputId": "e0b4a705-309a-40f9-fd0b-95775e8ce43e"
      },
      "source": [
        "# Correct the dataset directory to point to the 'images' folder\n",
        "dataset_dir_corrected = os.path.join(dataset_dir, 'images')\n",
        "\n",
        "# Load the training and validation datasets with the corrected path\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir_corrected,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir_corrected,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Get the class names again\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# Apply normalization\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Prefetch for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15000 files belonging to 1 classes.\n",
            "Using 12000 files for training.\n",
            "Found 15000 files belonging to 1 classes.\n",
            "Using 3000 files for validation.\n",
            "Class names: ['images']\n",
            "Number of classes: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e618db5"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the image dataset from \"/content/waste.zip\". This will involve extracting the zip file, organizing the data, and preparing it for use with TensorFlow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c4d7b7"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the dataset using `tf.keras.utils.image_dataset_from_directory` failed because the function only detected one class. This likely means the directory structure was not as expected (class subdirectories directly under the provided path). To proceed, I need to understand the actual directory structure within the `images` folder to correctly point `image_dataset_from_directory` to the location containing the class folders. I will list the contents of `/content/waste/waste/images` to verify the structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31a4f711",
        "outputId": "3f3fff43-e5c8-474d-81e4-6287df11de55"
      },
      "source": [
        "# List the contents of the 'images' directory\n",
        "!ls -l /content/waste/waste/images"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 32 root root 4096 Jul 24 12:50 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cfc94b9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `ls -l` command showed that the `/content/waste/waste/images` directory *itself* contains another directory also named `images`. This is unexpected and likely the reason why `image_dataset_from_directory` is not finding the class subdirectories. The actual class directories must be inside `/content/waste/waste/images/images`. I need to verify this hypothesis by listing the contents of `/content/waste/waste/images/images`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8498338",
        "outputId": "992a83d1-ec4b-403a-efa7-1bed6dbb2f7e"
      },
      "source": [
        "# List the contents of the nested 'images' directory\n",
        "!ls -l /content/waste/waste/images/images"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 120\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 aerosol_cans\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 aluminum_food_cans\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 aluminum_soda_cans\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 cardboard_boxes\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 cardboard_packaging\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 clothing\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 coffee_grounds\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 disposable_plastic_cutlery\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 eggshells\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 food_waste\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 glass_beverage_bottles\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 glass_cosmetic_containers\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 glass_food_jars\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 magazines\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 newspaper\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 office_paper\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 paper_cups\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_cup_lids\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_detergent_bottles\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_food_containers\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_shopping_bags\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_soda_bottles\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_straws\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_trash_bags\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 plastic_water_bottles\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 shoes\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 steel_food_cans\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 styrofoam_cups\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 styrofoam_food_containers\n",
            "drwxr-xr-x 4 root root 4096 Jul 24 12:50 tea_bags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34491645"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ls -l` output confirms that the class-specific subdirectories are located within `/content/waste/waste/images/images`. Now I can correctly load the training and validation datasets using `tf.keras.utils.image_dataset_from_directory` by providing this correct path. I will also apply the normalization layer and prefetch the datasets as instructed, and print the class names and count to verify the loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caa1ad27",
        "outputId": "aec0da4f-d52f-4372-aba8-b6addc41d3fb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define the corrected path to the dataset containing class subdirectories\n",
        "dataset_dir_corrected = '/content/waste/waste/images/images'\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir_corrected,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir_corrected,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "# Get the class names\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "\n",
        "# Apply normalization\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Prefetch for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15000 files belonging to 30 classes.\n",
            "Using 12000 files for training.\n",
            "Found 15000 files belonging to 30 classes.\n",
            "Using 3000 files for validation.\n",
            "Class names: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
            "Number of classes: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbedbd7b"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "### Subtask:\n",
        "Define a lightweight image classification model using TensorFlow. This will involve choosing a suitable architecture and configuring it for the recyclable item classification task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a944893"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary layers and the Sequential model from TensorFlow Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4488d7bf"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a22ebe6a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a lightweight CNN model using the imported layers and Sequential model, then print the model summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "7e474ebf",
        "outputId": "80c36d4f-7064-4be6-ba04-fa234e324eae"
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dense(units=output_units, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,605,760\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m3,870\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,870</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,633,214\u001b[0m (6.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,633,214</span> (6.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,633,214\u001b[0m (6.23 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,633,214</span> (6.23 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd926e2"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the defined model using the prepared dataset. This will involve compiling the model, setting up the training process, and monitoring its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fe6d94"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile and train the defined model using the specified optimizer, loss function, and metrics, and store the training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d68d0864",
        "outputId": "c7653b6b-58fa-403f-902c-e9bbe71b0b7e"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 15\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 453ms/step - accuracy: 0.9651 - loss: 0.1145 - val_accuracy: 0.6227 - val_loss: 3.5031\n",
            "Epoch 2/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 450ms/step - accuracy: 0.9689 - loss: 0.0924 - val_accuracy: 0.6307 - val_loss: 3.3947\n",
            "Epoch 3/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 454ms/step - accuracy: 0.9670 - loss: 0.1019 - val_accuracy: 0.6253 - val_loss: 3.4610\n",
            "Epoch 4/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 450ms/step - accuracy: 0.9663 - loss: 0.1078 - val_accuracy: 0.6273 - val_loss: 3.6362\n",
            "Epoch 5/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 445ms/step - accuracy: 0.9696 - loss: 0.0932 - val_accuracy: 0.6253 - val_loss: 3.6982\n",
            "Epoch 6/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 450ms/step - accuracy: 0.9735 - loss: 0.0800 - val_accuracy: 0.6293 - val_loss: 3.8757\n",
            "Epoch 7/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 451ms/step - accuracy: 0.9682 - loss: 0.0958 - val_accuracy: 0.6307 - val_loss: 3.6972\n",
            "Epoch 8/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 447ms/step - accuracy: 0.9723 - loss: 0.0870 - val_accuracy: 0.6257 - val_loss: 3.8517\n",
            "Epoch 9/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 473ms/step - accuracy: 0.9715 - loss: 0.0783 - val_accuracy: 0.6287 - val_loss: 3.9764\n",
            "Epoch 10/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 475ms/step - accuracy: 0.9703 - loss: 0.0858 - val_accuracy: 0.6193 - val_loss: 3.8699\n",
            "Epoch 11/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 449ms/step - accuracy: 0.9744 - loss: 0.0779 - val_accuracy: 0.6347 - val_loss: 3.8740\n",
            "Epoch 12/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 449ms/step - accuracy: 0.9742 - loss: 0.0765 - val_accuracy: 0.6260 - val_loss: 3.9153\n",
            "Epoch 13/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 447ms/step - accuracy: 0.9744 - loss: 0.0679 - val_accuracy: 0.6280 - val_loss: 3.7287\n",
            "Epoch 14/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 449ms/step - accuracy: 0.9757 - loss: 0.0689 - val_accuracy: 0.6307 - val_loss: 3.6038\n",
            "Epoch 15/15\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 445ms/step - accuracy: 0.9789 - loss: 0.0592 - val_accuracy: 0.6283 - val_loss: 3.9788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70c6d811"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on a test set to obtain accuracy metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178e8ae1"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model's performance on the validation dataset and print the evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be6b1b4f",
        "outputId": "2a1a0796-656d-44fa-cb46-86cda5c91123"
      },
      "source": [
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(f\"Validation Loss: {loss}\")\n",
        "print(f\"Validation Accuracy: {accuracy}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 129ms/step - accuracy: 0.6398 - loss: 3.8464\n",
            "Validation Loss: 3.978771448135376\n",
            "Validation Accuracy: 0.628333330154419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbe76c97"
      },
      "source": [
        "## Model Conversion to TensorFlow Lite\n",
        "\n",
        "### Subtask:\n",
        "Convert the trained model to TensorFlow Lite format for deployment on edge devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38707f86"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the trained Keras model to a TensorFlow Lite model using the `TFLiteConverter.from_keras_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b6a2a6d",
        "outputId": "05b2bff4-cedb-4915-8b0f-4b34ed956f4b"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a file\n",
        "with open('waste_classifier.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model converted to TensorFlow Lite and saved as 'waste_classifier.tflite'\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpbci1tldx'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='keras_tensor_22')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 30), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139780601582736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601592528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601590608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601591184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601590224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601589072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601592144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601589648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601586576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139780601587536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model converted to TensorFlow Lite and saved as 'waste_classifier.tflite'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b57da48"
      },
      "source": [
        "## TensorFlow Lite Model Testing\n",
        "\n",
        "### Subtask:\n",
        "Test the converted TensorFlow Lite model to ensure it works correctly and efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff8a03f"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the TensorFlow Lite model and create an interpreter. Get the input and output tensors and allocate tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83d01b68",
        "outputId": "cf65c8a4-b4e4-4819-ead8-b229bc8e9ad6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"waste_classifier.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(\"Input details:\", input_details)\n",
        "print(\"Output details:\", output_details)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input details: [{'name': 'serving_default_keras_tensor_22:0', 'index': 0, 'shape': array([  1, 128, 128,   3], dtype=int32), 'shape_signature': array([ -1, 128, 128,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Output details: [{'name': 'StatefulPartitionedCall_1:0', 'index': 21, 'shape': array([ 1, 30], dtype=int32), 'shape_signature': array([-1, 30], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67d3fd08"
      },
      "source": [
        "**Reasoning**:\n",
        "Get a sample image and label from the validation dataset, preprocess the image to match the model input requirements, and run inference with the TensorFlow Lite model. Then, get the prediction results and compare them with the true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43ad6a4",
        "outputId": "2cf1c983-7a32-4752-c20b-530f0db44673"
      },
      "source": [
        "# Get a sample image and label from the validation dataset\n",
        "for images, labels in val_ds.take(1):\n",
        "    sample_image = images[0]\n",
        "    sample_label = labels[0]\n",
        "\n",
        "# The model expects a batch of images, so add an extra dimension\n",
        "input_data = np.expand_dims(sample_image, axis=0)\n",
        "\n",
        "# Set the tensor to the input data\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "\n",
        "# Get the output tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# The output is a probability distribution, get the predicted class index\n",
        "predicted_class_index = np.argmax(output_data)\n",
        "\n",
        "# Get the true class index\n",
        "true_class_index = np.argmax(sample_label.numpy())\n",
        "\n",
        "# Get the class names (assuming class_names variable is available)\n",
        "predicted_class_name = class_names[predicted_class_index]\n",
        "true_class_name = class_names[true_class_index]\n",
        "\n",
        "\n",
        "print(f\"True Class: {true_class_name}\")\n",
        "print(f\"Predicted Class: {predicted_class_name}\")\n",
        "\n",
        "# You can optionally visualize the image\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(sample_image.numpy())\n",
        "# plt.title(f\"True: {true_class_name}, Predicted: {predicted_class_name}\")\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Class: plastic_trash_bags\n",
            "Predicted Class: plastic_trash_bags\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c57c902a"
      },
      "source": [
        "## Edge AI Explanation\n",
        "\n",
        "### Subtask:\n",
        "Explain the benefits of Edge AI for real-time applications, specifically in the context of this image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a451f83"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the benefits of Edge AI for real-time applications, including low latency, reduced bandwidth, enhanced privacy, and cost-effectiveness, relating them to the recyclable item classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a0e53b4"
      },
      "source": [
        "### Benefits of Edge AI for Real-Time Recyclable Item Classification\n",
        "\n",
        "Edge AI refers to running AI and machine learning models directly on a local device (like a smartphone, a smart camera, or an embedded system) rather than relying on cloud-based servers. For real-time recyclable item classification, deploying the TensorFlow Lite model to the edge offers significant advantages:\n",
        "\n",
        "*   **Low Latency:** Processing images directly on the device eliminates the time delay associated with sending data to the cloud and receiving a response. This is crucial for real-time applications where quick decisions are needed, such as sorting waste on a conveyor belt.\n",
        "*   **Reduced Bandwidth:** Edge AI reduces the need to constantly transmit large image files over the internet to a central server. This saves bandwidth, which can be important in environments with limited connectivity or high data costs.\n",
        "*   **Enhanced Privacy:** Processing data locally on the device can improve privacy, as sensitive image data does not need to be sent to the cloud for analysis.\n",
        "*   **Cost-Effectiveness:** While there might be an initial cost for edge hardware, running inference on the edge can be more cost-effective in the long run compared to the ongoing costs of cloud computing resources, especially for applications with high inference volume.\n",
        "*   **Offline Capability:** Edge AI allows the application to function even without a continuous internet connection, making it reliable in various operational environments.\n",
        "\n",
        "In the context of recyclable item classification, these benefits translate to a more responsive and efficient system. A device with an edge AI model can quickly identify and categorize waste items as they are presented, enabling faster sorting processes and potentially reducing contamination in recycling streams. This can be applied in various scenarios, from automated sorting facilities to smart bins or even mobile applications that help individuals identify recyclable materials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7295ce9b"
      },
      "source": [
        "## Report Generation\n",
        "\n",
        "### Subtask:\n",
        "Prepare a report summarizing the code, accuracy metrics, and deployment steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "225391f9"
      },
      "source": [
        "## Report: Lightweight Image Classification Model for Recyclable Items\n",
        "\n",
        "### Objective:\n",
        "Train a lightweight image classification model using TensorFlow Lite for recognizing recyclable items from the provided dataset, convert it to TensorFlow Lite, test its performance, and explain the benefits of Edge AI for this application.\n",
        "\n",
        "### Data Loading and Preprocessing:\n",
        "The dataset was loaded from \"/content/waste.zip\". After extracting the zip file, the images were found to be organized in subdirectories within `/content/waste/waste/images/images`, with each subdirectory representing a class. The `tf.keras.utils.image_dataset_from_directory` function was used to load the training and validation datasets, specifying the correct directory path, image size (128x128), and batch size (32). The datasets were normalized by rescaling the pixel values to the range [0, 1] and prefetched for performance.\n",
        "\n",
        "Number of classes found: 30\n",
        "Class names: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n",
        "\n",
        "### Model Definition:\n",
        "A lightweight Convolutional Neural Network (CNN) model was defined using TensorFlow Keras Sequential API. The model consists of three convolutional layers with MaxPooling layers, followed by a Flatten layer and two Dense layers. The final Dense layer has 30 units with a softmax activation for multi-class classification.\n",
        "\n",
        "Model Summary:\n",
        "(See Model Summary output from cell `7e474ebf`)\n",
        "\n",
        "### Model Training:\n",
        "The model was compiled with the Adam optimizer, categorical crossentropy loss function, and accuracy as the evaluation metric. The model was trained for 15 epochs using the prepared training and validation datasets.\n",
        "\n",
        "Training History:\n",
        "(See Training History output from cell `d68d0864`)\n",
        "\n",
        "### Model Evaluation:\n",
        "The trained model was evaluated on the validation dataset to obtain accuracy metrics.\n",
        "\n",
        "Validation Loss: 3.9788\n",
        "Validation Accuracy: 0.6283\n",
        "\n",
        "### Model Conversion to TensorFlow Lite:\n",
        "The trained Keras model was converted to TensorFlow Lite format using `tf.lite.TFLiteConverter.from_keras_model`. The converted model was saved to a file named `waste_classifier.tflite`.\n",
        "\n",
        "### TensorFlow Lite Model Testing:\n",
        "The converted TensorFlow Lite model was loaded using the `tf.lite.Interpreter`. A sample image from the validation dataset was used to perform inference with the TFLite model. The predicted class was compared with the true class to verify the model's functionality.\n",
        "\n",
        "Sample Test Result:\n",
        "True Class: plastic_trash_bags\n",
        "Predicted Class: plastic_trash_bags\n",
        "\n",
        "### Edge AI Explanation:\n",
        "Deploying this model to the edge offers several benefits for real-time recyclable item classification:\n",
        "\n",
        "*   **Low Latency:** Enables quick identification of waste items for faster sorting.\n",
        "*   **Reduced Bandwidth:** Decreases the need to send large image data to the cloud.\n",
        "*   **Enhanced Privacy:** Image data is processed locally, improving privacy.\n",
        "*   **Cost-Effectiveness:** Can be more cost-effective than cloud-based inference for high-volume applications.\n",
        "*   **Offline Capability:** Allows the system to function without continuous internet access.\n",
        "\n",
        "These benefits make Edge AI a suitable approach for real-time waste classification applications in various environments.\n",
        "\n",
        "### Deployment Steps:\n",
        "1.  **Obtain the TensorFlow Lite Model:** The converted model `waste_classifier.tflite` is ready for deployment.\n",
        "2.  **Choose an Edge Device:** Select a suitable edge device (e.g., smartphone, Raspberry Pi, embedded system) that supports TensorFlow Lite.\n",
        "3.  **Integrate the Model:** Integrate the `waste_classifier.tflite` model into your application on the chosen edge device using the TensorFlow Lite interpreter.\n",
        "4.  **Preprocessing:** Implement the same preprocessing steps (rescaling to [0, 1] and resizing to 128x128) for new images before feeding them to the TFLite model.\n",
        "5.  **Inference:** Run inference using the TFLite interpreter to get predictions for new images.\n",
        "6.  **Postprocessing:** Interpret the model's output (probability distribution) to determine the predicted class.\n",
        "7.  **Application Logic:** Implement the application logic based on the predicted class (e.g., control a sorting mechanism, display the class name to the user).\n",
        "\n",
        "This report summarizes the process of training and converting a lightweight image classification model for recyclable items to TensorFlow Lite and highlights the advantages of using Edge AI for this task. The trained model and the TensorFlow Lite file are available for further use and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ece934f"
      },
      "source": [
        "## Visualize Accuracy\n",
        "\n",
        "### Subtask:\n",
        "Visualize the training and validation accuracy of the model over epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0f509ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Plot the training and validation accuracy over epochs using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "7231bd3c",
        "outputId": "d2e620dd-9ddd-426c-8bf9-9d8cea713c1f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9300dd7d"
      },
      "source": [
        "The model's validation accuracy of around 62.8% indicates that there is room for improvement. Several factors could contribute to this inaccuracy:\n",
        "\n",
        "*   **Dataset Size and Variety:** While 15,000 images is a good starting point, the complexity of distinguishing between 30 classes of waste might require a larger and more diverse dataset. Variations in lighting, angles, and the state of the waste items can also impact performance.\n",
        "*   **Class Imbalance:** Some classes in the dataset might have significantly more images than others. This can lead to the model being biased towards the majority classes and performing poorly on minority classes.\n",
        "*   **Model Architecture Complexity:** The current lightweight CNN model, while suitable for edge deployment, might not be complex enough to capture the intricate features needed to differentiate between all 30 waste categories effectively. More complex architectures or pre-trained models might be necessary.\n",
        "*   **Hyperparameters:** The chosen hyperparameters (e.g., learning rate, batch size, number of epochs) might not be optimal for this specific dataset and model architecture.\n",
        "*   **Overfitting:** While the training accuracy is very high (around 97%), the validation accuracy is significantly lower. This suggests that the model might be overfitting to the training data and not generalizing well to unseen images. The lack of regularization and data augmentation in the initial training could contribute to this.\n",
        "*   **Image Resolution and Quality:** The image size of 128x128 pixels might be too low to preserve enough detail for accurate classification of some waste items, especially those with subtle differences.\n",
        "*   **Labeling Errors:** Inaccurate labels in the dataset can also lead to the model learning incorrect associations.\n",
        "\n",
        "To improve the model's accuracy, we can explore strategies such as data augmentation, trying different model architectures, hyperparameter tuning, and utilizing transfer learning."
      ]
    }
  ]
}